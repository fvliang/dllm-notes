# 目前dllm工作梳理(按照我理解的顺序)

## 0. 一些diffusion的背景知识(cs236课程学的)

- 一般是cv领域生成任务（生成任务核心目标：模拟采样出来样本的px=客观数据的px）
- 这个观点可以换个视角理解llm（因为本质也是生成模型），llm从一个前置的prompt推理到一个完整的问答自然语言序列 x（x = Cat(prompt, resp)），
这个x是要在客观的自然语言的分布中采样出来的（从这个视角训练集=客观的自然语言的分布中的若干样本）。
- 但自回归（AR）只是众多生成类范式中最简单的，也是最容易拓展到大规模训练和大参数量的，结果人们惊奇发现大参数量直接获得了性能的scaling law，成就了今天llm的成功。
- 而diffusion是除了AR之外的另一种范式。
- diffusion起源于能源模型（EBM），大概意思是对每个给定x的px建模，但是发现就算建模好了，怎么采样呢？难道在一个无限大的采样空间无限试错？
- 一波聪明人忽然想到，我不直接建模px，而是建模px的梯度（不知为啥，这玩意就叫score），也就是“我知道怎么从一个糟糕的x变到一个px高的x不就行了”
- 然后得到了SOTA的生成模型（起码在cv领域）：diffusion，基于score不断的让一个糟糕的x变成一个好的x（这个过程又叫：退火郎之万采样）
- DDPM（比较出名的diffusion论文）其实本质也是在退火郎之万采样（因为可以从理论上证明score的期望其实就是“噪音”，
那DDPM里面的去噪其实就和退火郎之万采样中沿着score优化x是一样的思想，yangsong博士证明过这个事）


## 1. SEDD: Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution

- 依旧来自stanford生成领域大牛Ermon课题组，是yangsong师弟的工作。
- 内部沿用了cv领域diffusion的score matching思想，完全迁移到了离散nlp任务上。
- 核心思想就是怎么让一个离散token的句子变的越来越乱，再从一个比较乱的句子中逐步恢复原来的样子。
- 此工作首次提出媲美ARM的离散扩散模型，分别在GPT2-small(124M)和GPT2-medium(355M)规模。
- 更像是一个把cv领域的score强硬的迁移到了离散nlp任务上，但起码work了，效果还不错，但似乎缺少了一些nlp的针对性设计。
- 而且规模这小，才0.1/0.3b参数量，只能说发现了dllm的潜力，获得了ICML24 best paper。
---

## 2. RADD: YOUR ABSORBING DISCRETE DIFFUSION SECRETLY MODELS THE CONDITIONAL DISTRIBUTIONS OF CLEAN DATA

- 出自人大高瓴团队，LLADA系列开篇之作，参数量估计和SEDD差不多，都是0.1/0.3b。
- 原来SEDD认为要在不同的t下得到不同的噪声预测， 因此需要|t|个不同的预测任务的神经网络，t一般要作为一个参数
- 因此旧dllm认为要使用diffusion-transformer，这个特殊的transformer不仅要接受token序列，还要接受t，但还是seq4seq任务，所以还是transformer
- RADD延续了SEDD的工作，把diffusion认为很重要的t隔离出来了,那不就训练一个单独针对p0的神经网络不就行了，而且不用把t作为参数，那不就成正常的transformer了，
而且好像有点BERT的意思，但又好像不大一样（MASK比例起码不是固定的）。
- 这样一来，训练似乎变得更加目标明确了：只需要训练p0（0时刻的clean data数据分布）的score值，pt的score值可以直接用p0乘一个t的标量得到。
- 训练的目标函数也由SEDD的丑陋版本变成了简化版本。
- 而且由于预测的任务和t无关，如果某一轮没有token更新（从M -> real_token）,那其实就不用再跑一遍了，用上一个的输出就行，算是推理的优化了。

---

## 3. LLADA：Large Language Diffusion Models

- 终于来到目前SOTA的dllm，参数量达到了1b和8b，相当于沿用了RADD的样子做大了参数量
- 剩下感觉没啥好说的了
